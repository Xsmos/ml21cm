{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 6000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])\n",
    "train_ds_normalized = torchvision.datasets.MNIST(\n",
    "    root=\".\", train=True, download=True, transform=transform\n",
    "        )\n",
    "train_ds_unnormalized = torchvision.datasets.MNIST(root=\".\", train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_ds_normalized, batch_size=batch_size, shuffle=True,\n",
    "# )\n",
    "train_loader = torch.utils.data.DataLoader(train_ds_unnormalized, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_nor = torch.Size([6000, 1, 28, 28])\n",
      "shape_unn = torch.Size([6000, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_normalized, _ = next(iter(torch.utils.data.DataLoader(train_ds_normalized, shuffle=True, batch_size=6000)))\n",
    "images_unnormal, _ = next(iter(torch.utils.data.DataLoader(train_ds_unnormalized, shuffle=True, batch_size=6000)))\n",
    "print(\"shape_nor =\", images_normalized.shape)\n",
    "print(\"shape_unn =\", images_unnormal.shape)\n",
    "plt.hist(images_unnormal.numpy().reshape(-1), label=\"unnormalized\", density=True, bins=10, histtype=\"step\")\n",
    "plt.hist(images_normalized.numpy().reshape(-1), label=\"normalized\", density=True, bins=20, histtype=\"step\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.imshow(np.transpose(images[0], axes=(1,2,0)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1,2,4),\n",
    "    channels = 1\n",
    ").to(device)\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    objective = \"pred_noise\",\n",
    "    image_size = 28,\n",
    "    timesteps = 500,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "def plt_sampled_images(savefig=\"diffusion_epoch.png\"):\n",
    "    sampled_images = diffusion.sample(64).cpu()\n",
    "    sampled_images = sampled_images.reshape(8,8,1,28,28)\n",
    "    fig, ax = plt.subplots(8,8, figsize=(8,8), dpi=200, constrained_layout=True)\n",
    "    fig.suptitle(f\"epoch {epoch:03d}/{num_epochs:03d}; step {step:03d}\")\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            # ax = plt.subplot(4,4,i+1)\n",
    "            ax[i][j].imshow(np.transpose(sampled_images[i][j], axes=(1,2,0)), \n",
    "    cmap = 'gray')\n",
    "            ax[i][j].axis('off')\n",
    "            # ax[i][j].set_xticks([])\n",
    "            # ax[i][j].set_yticks([])\n",
    "    # plt.tight_layout()\n",
    "    if savefig != False:\n",
    "        plt.savefig(savefig)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/storage/home/hcoda1/3/bxia34/scratch/diffusion.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ondemand-phoenix.pace.gatech.edu/storage/home/hcoda1/3/bxia34/scratch/diffusion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ondemand-phoenix.pace.gatech.edu/storage/home/hcoda1/3/bxia34/scratch/diffusion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ondemand-phoenix.pace.gatech.edu/storage/home/hcoda1/3/bxia34/scratch/diffusion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m step, (mnist_images, mnist_labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ondemand-phoenix.pace.gatech.edu/storage/home/hcoda1/3/bxia34/scratch/diffusion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         mnist_images \u001b[39m=\u001b[39m mnist_images\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ondemand-phoenix.pace.gatech.edu/storage/home/hcoda1/3/bxia34/scratch/diffusion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    163\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[0;32m--> 164\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for step, (mnist_images, mnist_labels) in enumerate(train_loader):\n",
    "        mnist_images = mnist_images.to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion(mnist_images).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch == 0 and step % 5 == 0:\n",
    "        #     plt_sampled_images(f\"diffusion_step{step:03d}.png\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, {batch_size} batches, {step+1} steps, cost {end_time-start_time:.3f} sec, Loss.: {loss}\")\n",
    "    \n",
    "    plt_sampled_images(f\"diffusion_epoch{epoch:03d}.png\")\n",
    "\n",
    "torch.save(model.state_dict(), \"diffusion.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
